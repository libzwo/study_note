分类和聚类的异同

> 同：找出数据集中样本之间的关系
>
> 异：分类已经标记好每个样本的标签，聚类不需要样本的标签，直接基于样本的特征和样本之间的关系来进行分组

![image-20200618140929570](/Users/apple/Library/Application Support/typora-user-images/image-20200618140929570.png)

​																	*绿色欧式距离 红黄蓝曼哈顿距离*



 min-max规范化

> (x-min(x))/(max(x)-min(x))

零均值规范化

> (x-mean(x))/std(x)

![image-20200618143452738](/Users/apple/Library/Application Support/typora-user-images/image-20200618143452738.png)![image-20200618143604255](/Users/apple/Library/Application Support/typora-user-images/image-20200618143604255.png)

![image-20200618143509925](/Users/apple/Library/Application Support/typora-user-images/image-20200618143509925.png)



![image-20200618143523421](/Users/apple/Library/Application Support/typora-user-images/image-20200618143523421.png)

![image-20200618143538726](/Users/apple/Library/Application Support/typora-user-images/image-20200618143538726.png)

- 决策树

  > 预剪枝是在决策树构建过程中剪枝，后剪枝是在决策树构建完后剪枝
  >
  > 决策树的构建就是一个不断分类，不断减小熵的过程
  >
  > ---
  >
  > 预剪枝就是在决策树构建的**过程中**，设置一个阀值，如果某个分类熵减的幅度小于阀值，那么就不剪枝
  >
  > > 后剪枝就是在决策树构建**完成后**，对拥有同样父节点的一组节点进行检查，如果合并之后熵增的幅度小于阀值，那么就进行合并
  > >
  > > **后剪枝算法举例**：
  > >
  > > Reduced-Error Pruning (REP, 错误率降低剪枝）
  > >
  > > Pessimistic Error Pruning (PEP, 悲观剪枝）
  > >
  > > Cost-Complexity Pruning(CCP, 代价复杂度剪枝)
  >
  > 无论是预剪枝还是后剪枝，最终的目的都是为了减少决策树过拟合的问题

- 贝叶斯分类

  >若是数据频繁更替那么就采用**惰性学习**的方式，先不进行任何训练，当受到预测请求时再根据当前数据进行估值。若对预测速度要求较高就采用**查表**的方式，对于给定数据集，将涉及到的所有概率估值算出来，等到请求的时候查表给出。若数据不断增加就采用**增量学习**，基于现有估值，对新样本涉及到的概率估值进行修正
  >
  >---
  >
  >拉普拉斯修正，分子加1，分母加类别的种类数 

- k最近邻

  > k的取值一般取奇数，k过小容易导致过拟合，训练集中的噪声太敏感，k过大容易导致欠拟合，过于平滑，不能有效反应数据集中类别之间的差别

- 组合分类

  > 组合分类成功的前提
  >
  > >基分类器应该是独立的
  > >
  > >基分类器应当好于随机猜想
  >
  > **bagging**(并行化)&**boosting**(序列化)
  >
  > - bagging的训练集是原始训练集中有放回选取的，各训练集之间独立，boosting的训练集不变，只是训练集中每个样例在分类器中权重发生变化
  > - bagging采用均匀取样，每个样例权重相等，boosting随错误率不断调整样例权值，错误率越大权重越大（为了更好的划分误分样例）
  > - bagging的所有预测函数权重相等，boosting对于每个弱分类器都有对应的权重。分类器误差越小权重越大
  > - bagging各个预测函数可以并行生成，boosting各个预测函数只能顺序生成，因为后一个模型要参照前一个模型
  > - bagging的典型算法**随机森林**   boosting的典型算法**Adaboost**

- 聚类

  > 内部度量：对比聚类算法产生的簇和已知样本类别
  >
  > 外部度量：对比衡量聚类结果与数据集自身分布情况
  >
  > 内部度量的评价标准
  >
  > > 簇内紧凑度
  > >
  > > 簇间分离度
  > >
  > > 轮廓系数（轮廓系数越大，聚类效果越好）
  >
  > ---
  >
  > k均值拓展方法
  >
  > > 球面k均值
  > >
  > > 模糊k均值
  > >
  > > 核k均值
  > >
  > > 在线k均值
  >
  > k均值局限性
  >
  > > 对初始化敏感
  > >
  > > 不同的初始化及k值可能得到不同的结果
  > >
  > > 不同的距离度量适用于特定的簇
  >
  > ---
  >
  > 凝聚层次聚类的链接度量
  >
  > >最短距离：容易把两个簇混合在一起，两个簇从全局来看是分开的，但由
  > >
  > >于中间有个别点比较近就被合并了；对噪声敏感
  > >
  > >最长距离：可能把属于一个簇的样本分开；从整体上比较接近，但由于个
  > >
  > >别点离得比较开，根据最长距离原则使得两个子集距离很大，从而没有被
  > >
  > >合并
  > >
  > >平均距离：相对中和了以上两种情况，用得比较多。为了进一步减小对噪
  > >
  > >声得敏感，可以用中值代替平均值
  >
  > ---
  >
  > DBSCAN算法
  >
  > > 优点
  > >
  > > > 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集
  > > >
  > > > 可以在聚类的同时发现异常点，对数据集中的异常点不敏感
  > > >
  > > > 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响
  > > >
  > > > 不需要输入划分聚类的个数
  > >
  > > 缺点
  > >
  > > > 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时对于超参数的选取困难，用DBSCAN聚类一般不适合
  > > >
  > > > 数据量增大时，要求的较大的内存支持，I/O消耗也很大，此时可以对搜索最近邻时建立KD树等方法来改进效率
  > > >
  > > > 算法聚类效果依赖于距离公式的选取,实际应用中常用欧氏距离,对于高维数据，存在“维数灾难”
  > >
  > > 超参数eps和MinPts的作用
  > >
  > > > eps越小，MinPts越大，则要求构成同一个簇的样本之间的密度越大；这可能导致很多大的簇被碎片化，同时把处于密度比较小的区域的样本点标记为噪声
  > > >
  > > > eps越大，MinPts 越小，则要求构成同一个簇的样本之间的密度较小；这可能导致一些距离较近的簇被合并在一起，同时不易识别与簇离得较近的噪声点

- 关联规则挖掘

  > 基本思路
  >
  > - **频繁项集**frequent itemset**挖掘**
  > - **生成高置信度的规则**
  >
  > **先验原理（Apirori）**：如果一个项集频繁，则其所有子集也频繁
  >
  > **反单调性**：如果一个项集非频繁，则其所有超集也非频繁

