# 关于损失函数的一点思考

> 2022.7.28 周裕杰



## 信息量

信息论的奠基人**香农**认为“信息是用来消除随机不确定性的东西”，信息量的大小和信息发生的概率成反相关，举个例子太阳东升西落，这件事情发生的概率是一，所以这是一句屁话，没有蕴含任何有用的信息，六月下雪了，这是一个很罕见的例子，所以这是一个稀有事件，蕴含了很大的信息量。

如果一件事情发生的概率为P，其信息量表示为：
$$
I(x)=-log(P(x))
$$

>*“—”表示信息量的大小和发生概率负相关，log则是因为当多个稀有事件一起发生的时候，它们之间的概率是积的关系而它们蕴含的信息量是和的关系*

## 熵

信息量是对于单一事件的衡量标准，如果我们想衡量一个时间序列或者是一个复杂事件，那么我们就需要知道所有信息量的期望，也就是熵。

信息论的熵可以表示为：
$$
H(X)=-\sum_{i=1}^{n}P(x_i)log(P(x_i))
$$
神经网络所做的事情，其实就是希望从已知的数据集中抽象出某种分布，并且使之尽可能的接近真实分布，那么遇到了一个新的问题，**我们应该如何衡量两个分布之间的相似程度呢？**

## KL散度

对于同一个随机变量X的独立的概率分布P(x)和Q(x)，我们一般使用KL散度来衡量**两个概率分布之间的相似程度**

KL散度可以表示为：
$$
D_{KL}(P||Q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)})
$$
然而对于这么一个式子是不好算的，我们可以推导出**KL散度=交叉熵-信息熵**

具体推导过程如下：
$$
D_{KL}(P||Q)=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)})\\
=\sum_{i=1}^np(x_i)log(p(x_i))-\sum_{i=1}^np(x_i)log(q(x_i))\\
=-\sum_{i=1}^np(x_i)log(q(x_i))-H(X)
$$


对于神经网络来说，数据集是给定的，所以H(x)其实相当于一个常数，我们最小化KL散度的效果和最小化交叉熵的效果是一样的，所以在**分类问题**中我们一般都用交叉熵来当作*loss*函数。

---

# Batch Nuclear-norm Maximization

先来介绍两个概念***discriminability***和***diversity***，discriminability指的是判别力，模型判别物体属于某个类别的置信度越高，模型的判别力就越高，diversity指的是多样性，数据集最终分成的类别数越多，多样性就越强。

基于熵的方法存在着副作用，将会导致预测多样性的减少，因为很容易将位于判别边界附近的属于少数类别的点错误划分到多数类别。

接下来我将从三个方面思考，是否可以用矩阵的核范数来取代交叉熵作为损失函数，从而达到更好的效果。

## Measuring Discriminability with F-norm

A是一个B*C大小的数据矩阵，B代表样本数，C代表分到每个类别的概率

首先最基本的根据概率的非负性我们有：
$$
A_{i,j}\geq0\quad\forall{i\in{1,2,...,B},j\in{1,2,...,C}}
$$
根据一个事件的所有情况概率之和为1我们有：
$$
\sum_{j=1}^CA_{i,j}=1\quad\forall{i\in{1,2,...,B}}
$$
数据矩阵A的熵表示为：
$$
H(A)=-\frac{1}{B}\sum_{i=1}^B\sum_{j=1}^CA_{i,j}log(A_{i,j})
$$

> 当H(A)达到最小值时，A的每一行中只有一个列为1，其他C-1个列都是0，H(A)取到最小值恰好满足A的最高判别力，预测的每个样本都是完全确定的

数据矩阵A的F范数表示为：
$$
\Vert{A}\Vert{_F}=\sqrt{\sum_{i=1}^B\sum_{j=1}^C\vert{A_{i,j}}\vert^2}
$$
我们发现H(A)和$\Vert{A}\Vert{_F}$具有严格相反的单调性，并且H(A)的最小值点和$\Vert{A}\Vert{_F}$的最大值点是相同的

接下来我们来寻找$\Vert{A}\Vert{_F}$的上界：
$$
\Vert{A}\Vert{_F}\leq\sqrt{\sum_{i=1}^B(\sum_{j=1}^CA_{i,j})\cdot(\sum_{j=1}^CA_{i,j})}\\
=\sqrt{\sum_{i=1}^B1\cdot1}=\sqrt{B}
$$

> 最小化H(A)和最大化$\Vert{A}\Vert{_F}$可以在同一个点得到，而且最大化$\Vert{A}\Vert{_F}$也可以增强判别力

## Measuring Diversity with Matrix Rank

在实际情况中，我们随机选择一批样本，在这批样本中某些类别占主导地位，其他类别包含的样本数量较少甚至没有这是很常见的情况，如果我们仅仅使用熵最小化或者是最大化核范数，模型很容易将决策边界附近属于少数类别的样本误划分到多数类别，这不仅仅降低了模型预测的多样性，对于整体的准确性也是有害的。

现在让我们来思考上面$\Vert{A}\Vert{_F}$取到上界$\sqrt{B}$的情形，我们会发现，在这种情况下我们可以近似的将A中预测类别数量近似看作rank(A)，因此我们可以**通过最大化rank(A)来保持预测多样性**。

> 一个很简单的推导

## Batch Nuclear-norm Maximization

$$
\frac{1}{\sqrt{D}}\Vert{A}\Vert{_\star}\leq\Vert{A}\Vert_{F}\leq\Vert{A}\Vert{_\star}\leq\sqrt{D}\cdot\Vert{A}\Vert{_F}
$$



![WechatIMG54](https://s2.loli.net/2022/07/28/SFklvuciOGdUnqg.jpg)
$$
\Vert{A}\Vert{_\star}\leq\sqrt{D}\cdot\Vert{A}\Vert{_F}\leq\sqrt{D\cdot{B}}
$$
![WechatIMG57](https://s2.loli.net/2022/07/28/Km6Sfao4zHdrPxX.jpg)

正确分类的损失函数为：
$$
\ell_{cls}=\frac{1}{B_L}\Vert{Y^Llog(G(X^L))\Vert{_1}}
$$

> 上标下标L代表的是有标签的域选出来的样本，$Y^L$和$G(X^L)$是两个行向量，进行点积然后再平均求和

BNM的损失函数为：
$$
\ell_{bnm}=-\frac{1}{B_U}\Vert{G(X^U)}\Vert{_\star}
$$

> 上标下标U代表的是无标签的样本，目的是最大化核范数

总的损失函数为：
$$
\ell_{all}=\frac{1}{B_L}\Vert{Y^Llog(G(X^L))}\Vert{_1}-\frac{\lambda}{B_U}\Vert{G(X^U)}\Vert{_\star}
$$
